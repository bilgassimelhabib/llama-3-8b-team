{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "team-llama-validation"
   },
   "source": [
    "# ğŸ¦™ LLaMA 3 8B - Validation Ã‰quipe\\n",
    "**Notebook de test et validation sur Google Colab**\\n",
    "\\n",
    "ğŸ‘¥ **Ã‰quipe de dÃ©veloppement** - Synchronisation avec AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-environment"
   },
   "source": [
    "## ğŸ”§ 1. Configuration de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Installation des dÃ©pendances\\n",
    "!pip install -q torch>=2.1.0 transformers>=4.35.0 accelerate>=0.24.0 bitsandbytes>=0.41.0 huggingface-hub>=0.19.0 gradio>=4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Montage Google Drive pour la persistance\\n",
    "from google.colab import drive\\n",
    "drive.mount('/content/drive')\\n",
    "\\n",
    "print(\\\"âœ… Google Drive montÃ©\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sync-code"
   },
   "source": [
    "## ğŸ”„ 2. Synchronisation du Code depuis AWS/GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sync-options"
   },
   "outputs": [],
   "source": [
    "# CHOISISSEZ UNE OPTION :\\n",
    "\\n",
    "print(\\\"Options de synchronisation :\\\")\\n",
    "print(\\\"1. Depuis GitHub (recommandÃ©)\\\")\\n",
    "print(\\\"2. Depuis Google Drive (si vous avez sync manuel)\\\")\\n",
    "print(\\\"3. TÃ©lÃ©chargement direct\\\")\\n",
    "\\n",
    "option = 1  # â† Changez selon votre prÃ©fÃ©rence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sync-from-github"
   },
   "outputs": [],
   "source": [
    "if option == 1:\\n",
    "    # Synchronisation depuis GitHub\\n",
    "    !rm -rf llama-project\\n",
    "    !git clone https://github.com/VOTRE_USERNAME/llama-3-8b-team.git llama-project\\n",
    "    %cd llama-project\\n",
    "    print(\\\"âœ… Code synchronisÃ© depuis GitHub\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hardware-check"
   },
   "source": [
    "## ğŸ–¥ï¸ 3. VÃ©rification du MatÃ©riel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\\n",
    "import psutil\\n",
    "\\n",
    "print(\\\"ğŸ–¥ï¸ INFORMATIONS MATÃ‰RIEL COLAB\\\")\\n",
    "print(\\\"=\\\" * 50)\\n",
    "\\n",
    "# CPU\\n",
    "print(f\\\"âœ… CPU: {torch.get_num_threads()} cores\\\")\\n",
    "print(f\\\"âœ… RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\\\")\\n",
    "\\n",
    "# GPU\\n",
    "if torch.cuda.is_available():\\n",
    "    gpu_name = torch.cuda.get_device_name(0)\\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\\n",
    "    print(f\\\"âœ… GPU: {gpu_name}\\\")\\n",
    "    print(f\\\"âœ… VRAM: {gpu_memory:.1f} GB\\\")\\n",
    "else:\\n",
    "    print(\\\"âŒ Aucun GPU dÃ©tectÃ© - VÃ©rifiez la configuration Colab\\\")\\n",
    "\\n",
    "print(\\\"\\\\nğŸ¯ Convient pour LLaMA 3 8B (4-bit): OUI\\\" if torch.cuda.is_available() and gpu_memory >= 12 else \\\"\\\\nâš ï¸  GPU limitÃ© pour LLaMA 3 8B\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-model"
   },
   "source": [
    "## ğŸ§ª 4. Test du ModÃ¨le LLaMA 3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import-code"
   },
   "outputs": [],
   "source": [
    "# Importation de notre code\\n",
    "import sys\\n",
    "sys.path.append('/content/llama-project/src')\\n",
    "\\n",
    "from model_loader import ModelLoader\\n",
    "\\n",
    "print(\\\"âœ… Code de l'Ã©quipe importÃ©\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load-model"
   },
   "outputs": [],
   "source": [
    "# Chargement du modÃ¨le avec 4-bit quantization\\n",
    "print(\\\"ğŸ”„ Chargement de LLaMA 3 8B...\\\")\\n",
    "\\n",
    "loader = ModelLoader()\\n",
    "model, tokenizer = loader.load_model(quantize_4bit=True)\\n",
    "\\n",
    "print(\\\"âœ… ModÃ¨le chargÃ© avec succÃ¨s!\\\")\\n",
    "print(f\\\"ğŸ“Š ModÃ¨le sur: {next(model.parameters()).device}\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance-tests"
   },
   "source": [
    "## ğŸ“Š 5. Tests de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-benchmark"
   },
   "outputs": [],
   "source": [
    "import time\\n",
    "\\n",
    "def benchmark_generation(prompts, num_runs=2):\\n",
    "    results = []\\n",
    "    \\n",
    "    for i, prompt in enumerate(prompts):\\n",
    "        print(f\\\"\\\\nğŸ§ª Test {i+1}/{len(prompts)}: '{prompt[:50]}...'\\\")\\n",
    "        \\n",
    "        times = []\\n",
    "        for run in range(num_runs):\\n",
    "            start_time = time.time()\\n",
    "            \\n",
    "            response = loader.generate_text(\\n",
    "                prompt,\\n",
    "                max_length=200,\\n",
    "                temperature=0.7\\n",
    "            )\\n",
    "            \\n",
    "            generation_time = time.time() - start_time\\n",
    "            times.append(generation_time)\\n",
    "            \\n",
    "            if run == 0:  # Afficher seulement la premiÃ¨re rÃ©ponse\\n",
    "                print(f\\\"   RÃ©ponse: {response}\\\")\\n",
    "        \\n",
    "        avg_time = sum(times) / len(times)\\n",
    "        results.append({\\n",
    "            'prompt': prompt,\\n",
    "            'avg_time': avg_time,\\n",
    "            'tokens_per_second': len(response.split()) / avg_time\\n",
    "        })\\n",
    "        \\n",
    "        print(f\\\"   â±ï¸  Temps moyen: {avg_time:.2f}s\\\")\\n",
    "        print(f\\\"   ğŸš€ Tokens/seconde: {len(response.split()) / avg_time:.1f}\\\")\\n",
    "    \\n",
    "    return results\\n",
    "\\n",
    "# Tests de performance\\n",
    "test_prompts = [\\n",
    "    \\\"Explique-moi l'apprentissage automatique en une phrase:\\\",\\n",
    "    \\\"Qu'est-ce que la transformÃ©e de Fourier? RÃ©ponse courte:\\\",\\n",
    "    \\\"Donne-moi 3 conseils pour amÃ©liorer mon code Python:\\\"\\n",
    "]\\n",
    "\\n",
    "print(\\\"ğŸ“Š LANCEMENT DES TESTS DE PERFORMANCE\\\")\\n",
    "print(\\\"=\\\" * 50)\\n",
    "\\n",
    "performance_results = benchmark_generation(test_prompts, num_runs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "results-summary"
   },
   "outputs": [],
   "source": [
    "# RÃ©sumÃ© des performances\\n",
    "print(\\\"\\\\nğŸ“ˆ RÃ‰SUMÃ‰ DES PERFORMANCES\\\")\\n",
    "print(\\\"=\\\" * 40)\\n",
    "\\n",
    "avg_generation_time = sum(r['avg_time'] for r in performance_results) / len(performance_results)\\n",
    "avg_tokens_per_second = sum(r['tokens_per_second'] for r in performance_results) / len(performance_results)\\n",
    "\\n",
    "print(f\\\"â±ï¸  Temps de gÃ©nÃ©ration moyen: {avg_generation_time:.2f}s\\\")\\n",
    "print(f\\\"ğŸš€ Tokens/seconde moyen: {avg_tokens_per_second:.1f}\\\")\\n",
    "print(f\\\"ğŸ’¾ MÃ©moire GPU utilisÃ©e: {torch.cuda.memory_allocated() / 1e9:.1f} GB\\\")\\n",
    "\\n",
    "# Ã‰valuation\\n",
    "if avg_tokens_per_second > 5:\\n",
    "    print(\\\"\\\\nğŸ‰ PERFORMANCES: EXCELLENTES\\\")\\n",
    "elif avg_tokens_per_second > 2:\\n",
    "    print(\\\"\\\\nâœ… PERFORMANCES: BONNES\\\")\\n",
    "else:\\n",
    "    print(\\\"\\\\nâš ï¸  PERFORMANCES: LIMITÃ‰ES\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save-results"
   },
   "source": [
    "## ğŸ’¾ 6. Sauvegarde des RÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-to-drive"
   },
   "outputs": [],
   "source": [
    "# Sauvegarde des rÃ©sultats\\n",
    "import json\\n",
    "from datetime import datetime\\n",
    "\\n",
    "# CrÃ©ation du rapport\\n",
    "validation_report = {\\n",
    "    \\\"date\\\": datetime.now().isoformat(),\\n",
    "    \\\"environment\\\": {\\n",
    "        \\\"gpu\\\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \\\"None\\\",\\n",
    "        \\\"gpu_memory_gb\\\": torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0,\\n",
    "        \\\"colab_runtime\\\": \\\"GPU\\\" if torch.cuda.is_available() else \\\"CPU\\\"\\n",
    "    },\\n",
    "    \\\"performance\\\": performance_results,\\n",
    "    \\\"summary\\\": {\\n",
    "        \\\"avg_generation_time\\\": avg_generation_time,\\n",
    "        \\\"avg_tokens_per_second\\\": avg_tokens_per_second,\\n",
    "        \\\"gpu_memory_used_gb\\\": torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\\n",
    "    },\\n",
    "    \\\"validation_status\\\": \\\"SUCCESS\\\" if avg_tokens_per_second > 2 else \\\"WARNING\\\"\\n",
    "}\\n",
    "\\n",
    "# Sauvegarde sur Google Drive\\n",
    "report_path = \\\"/content/drive/MyDrive/llama_team_validation.json\\\"\\n",
    "with open(report_path, 'w') as f:\\n",
    "    json.dump(validation_report, f, indent=2)\\n",
    "\\n",
    "print(f\\\"âœ… Rapport sauvegardÃ©: {report_path}\\\")\\n",
    "print(f\\\"ğŸ“Š Statut validation: {validation_report['validation_status']}\\\")\\n",
    "\\n",
    "# Affichage du rapport\\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\"*50)\\n",
    "print(\\\"ğŸ‰ VALIDATION TERMINÃ‰E AVEC SUCCÃˆS!\\\")\\n",
    "print(\\\"=\\\"*50)\\n",
    "print(f\\\"ğŸ“… Date: {validation_report['date'][:19]}\\\")\\n",
    "print(f\\\"ğŸ–¥ï¸  Environment: {validation_report['environment']['colab_runtime']}\\\")\\n",
    "print(f\\\"â±ï¸  Performance: {validation_report['summary']['avg_tokens_per_second']:.1f} tokens/sec\\\")\\n",
    "print(f\\\"ğŸ’¾ MÃ©moire: {validation_report['summary']['gpu_memory_used_gb']:.1f} GB\\\")\\n",
    "print(\\\"\\\\nğŸš€ PrÃªt pour le dÃ©veloppement en Ã©quipe!\\\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
