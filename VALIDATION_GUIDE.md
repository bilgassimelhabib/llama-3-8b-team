# ðŸ§ª Guide de Validation Colab - Ã‰quipe LLaMA

## ðŸš€ Validation Express (5 minutes)

**Sur Colab, collez ceci dans une cellule :**

\`\`\`python
# ðŸš€ VALIDATION EXPRESS - Ã‰quipe LLaMA
!pip install -q torch transformers accelerate bitsandbytes huggingface-hub
!git clone https://github.com/VOTRE_USERNAME/llama-3-8b-team.git
%cd llama-3-8b-team

import sys
sys.path.append('/content/llama-3-8b-team/src')

from model_loader import ModelLoader
loader = ModelLoader()
model, tokenizer = loader.load_model(quantize_4bit=True)

response = loader.generate_text("Bonjour! Confirme que tu fonctionnes:")
print("ðŸ¤–", response)
\`\`\`

## ðŸ“Š Validation ComplÃ¨te

1. **TÃ©lÃ©chargez le notebook** depuis AWS :
   \`\`\`bash
   # Sur votre machine locale
   scp dev1@IP_AWS:~/llama-project/notebooks/llama3_validation_colab.ipynb .
   \`\`\`

2. **Upload sur Colab** : 
   - Allez sur [colab.research.google.com](https://colab.research.google.com)
   - Fichier â†’ Upload notebook
   - SÃ©lectionnez le fichier tÃ©lÃ©chargÃ©

3. **Suivez les Ã©tapes** dans le notebook

## ðŸ“ˆ MÃ©triques de SuccÃ¨s

| MÃ©trique | Minimum | IdÃ©al |
|----------|---------|--------|
| Tokens/sec | 2 | 5+ |
| MÃ©moire GPU | < 10GB | < 8GB |

## ðŸ› DÃ©pannage

**ProblÃ¨me** : Out of Memory
**Solution** : Utilisez \`quantize_4bit=True\`

**ProblÃ¨me** : GitHub inaccessible  
**Solution** : Utilisez l'option Google Drive dans le notebook

## ðŸ“ Rapport de Validation

Chaque membre doit remplir :
\`\`\`markdown
## Rapport - [Votre Nom]
- **Date** : [date]
- **GPU Colab** : [T4/P100/Other]
- **Performance** : [X] tokens/sec
- **Statut** : âœ… VALIDÃ‰ / âŒ Ã‰CHEC
\`\`\`

**Bon courage Ã  toute l'Ã©quipe!** ðŸ¦™ðŸš€
